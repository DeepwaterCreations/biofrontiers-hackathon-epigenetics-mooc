Epigenetics MOOC Notes
BioFrontiers Hackathon, May 22-24, 2017

Author: Bridget Menasche
Last update: 2017-05-24 16:52


Data set	2
From Slack channel	2
From our initial discussion	2
Data structure	3
Structure:	3
Details	3
Questions	3
Question points	4
Question 1	4
Question 2	4
Question 3	4
Question 4	5
Ways to use this data?	5
Project goals and main research questions	5
Data processing	6
Concepts	6
Workflow	6
Potential data features	8
Code Notes	8
Load_json.py	8
output_csv.py	9
Epigenetics-Answer-Classifier.ipynb on 5/23/17	9
Epigenetics-Answer-Classifier.ipynb on 5/24/17	9
Notes and questions about the code	11
Project steps	12
Natural language processing methods	12
spaCy	12
spaCy resources	13
Neural Network	14
Punch cards	14
Completed tasks	14
Todo	14

Data set
From Slack channel
Briefly, we are going to use the materials from the Coursera course “Epigenetic Control of Gene Expression” https://www.coursera.org/learn/epigenetics by the University of Melbourne to assess effective human learning of highly complex and abstract scientific content. I will share my course materials from 2 years ago in an archive. Please, don’t share them beyond this team. Alternatively, you can sign up on Coursera _for free_. All the materials except the assignments are accessible.

The main dataset, which was kindly shared with us by U of Melbourne after a year of negotiation and ethics review, will be shared separately at the hackathon. It consists of the final writing assignment (Week 7) for three editions of the course and is structured as follows:
    • Two papers as _ground-truth_ reading
    • Very detailed questions on the reading, complete with an exhaustive list of sub-topics that have to be addressed
    • Student answers (for about 7,000 students, which is _a lot_ for a BioNLP dataset)
    • One-line precise answers to the questions by the expert staff, and full guidelines for the peer reviewers
    • 4+ reviews for each answer in the form of a score of 0, 1, or 2.
    • Reviewer comments, if any

From our initial discussion
Dataset is the last assignment: students read two additional papers, a scientific paper about epigenetic factors in cancer, the other is a broad, economist style for the public article. Each student has to answer certain questions in the writing assignment; these are open-ended paragraphs of text. Questions are very specific and include info about what should be mentioned in the answers. After this ends, each student has to peer review four other people’s answers. Students get the official answers to the questions and a rubric for reviewing.
0, doesn’t answer the question
1, answers the question but it’s not very clear
2, answers the question well
And there’s space for students to add comments once they do the grades.

We need to create a classifier that can handle these three labels.

The dataset contains writing assignment data from the first 3 instances of the course, from 7000 students that have completed the writing assignment - and though this is a small fraction of the number of people that took the course, it’s still a big dataset for the kind of analysis. Interesting because there are several levels of feedback that immediately suggest supervised learning and training of a classifier. That’s the main thing we want to do and the achievable thing in this project.
Data structure
Data we’re starting with, the assignment answers, does not include the background material used in the course, though since we have that can potentially include background data later.
Each question has multiple points. The student writes an answer in sentence/paragraphs. But it’s not the paragraph as a whole that gets scored - individual points within the paragraphs are separately evaluated by peer reviewers who are provided with the correct answer for that individual point.

See the answers and evaluation pdf document for example answers and evaluations of those answers. 
In order to create and train a classifier need to pair questions, answers, and scores.

Structure:
-	Each question has subquestions/guidelines (“you should mention this fact as part of your answer”). 
-	For each one of those subquestions, we have the correct answer, the student’s answer, and about 4 separate reviews for the answer to each subquestion. 
-	Not everyone has the same number of reviews, there should be some coverage of everyone from peer review however.
-	Each of the subquestions will then have an average score associated with it
-	Note that though this structure exists conceptually, it doesn’t exist explicitly within the data. A given question has defined subquestions, defined individual answers, and defined individual scores. But the student input text is one block - a paragraph or set of paragraphs. 
-	Seems like one challenge will be breaking up

Details
Questions
There are 4 (four) questions:
1.	Describe how DNA methylation is altered in cancer. In your answer include the following points 
2.	Describe how disruption of imprinting can contribute to cancer, using the example of the H19/Igf2 cluster. In your answer include the following points 
3.	The Economist article “Cancer’s epicentre (http://www.economist.com/node/21552168)” describes several drugs that affect epigenetic processes. Explain how Decitabine may be used to treat cancer, with reference to effects on the epigenome. In your answer include the following points
4.	Dr Stephen Baylin speculates in the Economist article that "epigenetic drugs altered the tumour cells in some lasting way that made them more susceptible to standard chemotherapy." How can drugs that alter DNA methylation have effects that last beyond the period of drug treatment? Discuss whether there are any periods of development when you would avoid treating patients with such drugs. In your answer include the following points 


Question points
1.	Question 1
“Describe how DNA methylation is altered in cancer. In your answer include the following points:”
1.1.	Describe the normal function of DNA methylation at CpG islands.
1.2.	Describe how DNA methylation of CpG islands is disrupted in cancer.
1.3.	Explain how disruption of DNA methylation at CpG islands contributes to cancer.
1.4.	Describe the normal function of DNA methylation in intergenic regions and repetitive elements. 
1.5.	Describe how DNA methylation in intergenic regions and repetitive elements is disrupted in cancer. 
1.6.	Explain how disruption of DNA methylation in intergenic regions and repetitive elements contributes to cancer. 

2.	Question 2
Describe how disruption of imprinting can contribute to cancer, using the example of the H19/Igf2 cluster. In your answer include the following points

2.1.	Describe the methylation pattern of the paternal allele and how this determines Igf2 expression status.
2.2.	Describe the methylation pattern of the maternal allele and how this determines Igf2 expression status.					
2.3.	Describe how imprinting at the H19/Igf2 cluster is disrupted in Wilm’s tumour. 
2.4.	Explain how disrupting imprinting at the H19/Igf2 cluster contributes to cancer. 

3.	Question 3
The Economist article “Cancer’s epicentre (http://www.economist.com/node/21552168)” describes several drugs that affect epigenetic processes. Explain how Decitabine may be used to treat cancer, with reference to effects on the epigenome. In your answer include the following points 

3.1.	Identify the class of epigenetic inhibitors that Decitabine belongs to. 
3.2.	Describe the impact of Decitabine on DNA methylation.
3.3.	Describe how Decitabine can have an anti-tumour effect. 

4.	Question 4
Dr Stephen Baylin speculates in the Economist article that "epigenetic drugs altered the tumour cells in some lasting way that made them more susceptible to standard chemotherapy." How can drugs that alter DNA methylation have effects that last beyond the period of drug treatment? Discuss whether there are any periods of development when you would avoid treating patients with such drugs. In your answer include the following points 

4.1.	Describe how altering DNA methylation can have enduring effects on the epigenome. 
4.2.	Define what is meant by a sensitive period.
4.3.	Identify sensitive periods of development.
4.4.	Explain why treating patients during sensitive periods would be inadvisable. 



Ways to use this data?
-	Can generate an overall score for an answer based on the score stuff in the data - the peer review scores correspond to individual points, but we can add those up within a peer review and then average them to get an aggregate score for each answer.
-	We can use the defined answers for each subquestion as features within the input data, the text paragraph student answer.
-	Bag of words approach: will have a distribution of all the important words in the paragraph (not and, to, etc). Will convert the text paragraph into a vector. Then this serves as one feature.
-	If we’re able to expand on the bag of words approach, is it possible to look for which words occur together in a sentence?

Project goals and main research questions


Question: how are we going to assess effectiveness of learning? We have training data of good answers/bad answers. Make it clear when writing the final version if this training data is from student scores only, or from instructor reading/scores of answers. In addition, while our project will probably depend just on the course data, we should address in the intro and discussion how the assignment structure, the evaluation structure, and the scores, connect to broader ideas about effectiveness of learning. 

Goal is to create a piece of software that grades answers.
Later work, not included here, will include epigenetics background info, and other models.



Data processing 
Concepts
Anything that’s trained to parse scientific publications is not appropriate for analyzing this data set since it comes from a MOOC - these are answers written by the “general public”, so even very good answers that represent understanding, that are correct, will be written in a different style than standard scientific language.

Going to base the training features on shallow semantics: sentence level ngram models/sentence level data. Narrative features are in the middle and generally not included in shallow semantics. Syntactic analysis included in shallow semantics (parts of speech, parsing a single sentence, etc). Doesn’t come close to human-term understanding of a sentence … it is statistical analysis of a text. Deep semantics relies on more elaborate models of thinking and learning or of knowledge representation. Neural networks can give you a great classifier and you have know idea why it’s great. Trade-off or dichotomy between inference and prediction. Neural networks can “replicate but not explain” according to Adolfo.

Can subject the text to preprocessing to generate certain features - a neural network would need to be very detailed to extract features itself, and that would take too much time and effort. So we’ll do this partially “by hand”?
Ontological annotation is outside of our scope (look this up so I can explain it in the paper)

The input is up to us: do we make our own model of the knowledge that the students are supposed to internalize and use to answer the questions?

Not trivial to take scientific articles and create computable knowledge out of that (is that something a more complete version of this project would want to do?). So here the goal is not to go from the papers provided as input in the course to

Set up several instances of tensor flow and compare them - this is a type of neural network - want something that’s easily accessible and that we can do iterations with, since iterations are important for this kind of project.
(not sure what tensor flow is, look it up so I can explain that in the paper)


Workflow

Main questions we’re working with right now to organize the project:
How do we parse the input or code or structure the input paragraph data so that it can be analyzed

Possible order of operations:
Pre-process data
Extract features from text
Decide on the scoring based on the features that we chose/that the program chose
Design and train neural network
Iterate over these operations again to perhaps add features that we want to look at


Goals for today:
Pre-process data
Extract features from text
(what is meaningful? Working from words or from engrams? Maybe we’re going to start with words to get the first version working)

Probably going to use question 1 as a prototype for the

Can we develop a classifier than can extract from the student answer text evaluative features to tell whether this is a good answer or not?


(One thought, not discussed in the group: how to sentence structures/writing skills correlate with understanding?)

(Another thing discussed: some of the answers read like they were written in another language and then run through google translate, and that can maybe create issues in accurate scoring. However the assignment and scoring instructions include this:						
“Please remember that not all students come from an English speaking background. Focus on whether the relevant points have been clearly stated and not on the quality and sophistication of the language.”)

May need to start with the data as a block from all 3 years, then break it up in the future. Consistency between the questions isn’t an issue however - the course is the same over those years.

Validation: want to condense the official answers into a paragraph and then run that through the classifier to make sure it scores them as correct.

Data: JSON
Mostly processed. Need to remove HTML code for paragraphs breaks for example.
Format:

Format we might want it in:
Answer vector (bag of words and/or phrases)
Evaluation vector (scalar; for question 1, from 0-12)


The answer vector might be extremely long … each word or phrase we’re looking for can be a position in the vector, and can be represented as a 1 for presence of that word or 0 as absence of the word. Can also include numbers of how many times this word shows up. Again, this would be a single feature, and we can just train the classifier on that feature. If using counts of how many times each word shows up, will need to normalize for total word count (or some other metric of length) for the answer.

Maybe we’ll start with the official answer to create the desired answer vector

Once this is done we’ll think about other features that we can use and add to the model. Some will be predictive, others not, and we can do analysis of which of those are useful.

Data input into neural network: needs to be in csv format, where each line represents an answer, and contains the vector representing a feature for that answer along with the aggregated score for that answer.
Potential data features
1.	Bag of words: incidence only
2.	Bag of words: word counts, normalized by total number of words in that answer
3.	Triples: subject, verb, object of each sentence
4.	Answer length: total number of words
5.	Answer length: total number of sentences

Note: features that reflect the length of the answer could serve as useful controls. The length of an answer likely correlates with the evaluation of the answer, but length does not necessarily reflect understanding or cause an answer to be correct. It might be necessary to somehow control for answer length in order to remove that as a factor when doing deeper analysis of this data. For instance, it might be useful to sort the data into length categories ( < x words, x-y words, > y words) and then re-run the classifier within those categories to make sure it is still predictive. This could be done for the training step, the evaluation step, or both.


Code Notes

Load_json.py
Requirements: import sys
import json
from html.parser import HTMLParser
import numpy as np

This script loads the data already in JSON format and outputs a list of sublists, where each sublist is a student's answer to one question followed by the averages of the scores given to that question by the student's reviewers. Since this data set contained multiple questions, this script separates the data answer and score data for each question. In addition, it removes reviewer comments associated with a student’s answer as well as leftover HTML code still in the JSON formatted file. The script normalizes the scores for each answer and returns scores between 0 and 1 for analysis. It also outputs features in the format (x, y) where x is a list of answers to a question, and y is a list of sums over the averaged score array for that answer.

This script was written for the structure of the input JSON data used for this paper. The script separates out answers with the associated scores, and this separation is based on known association with the scores in the list to the relevant question and answer. When using datasets with different numbers of scores per answer, or a different structure, the details of this script will need to be modified in order to tackle the different dataset. Output from this script is a python list of student answers and a python list of scores for those answers.

output_csv.py
Turns the json structure into csv rows. Useful for human exploration of the data and asking questions and creating hypotheses for further investigation. Not input into spaCy or the neural network.

Epigenetics-Answer-Classifier.ipynb on 5/23/17
This script creates a vocabulary for the analysis by listing the most common 10,000 words in the input data set. Then takes a string of words (answer) as input and returns a vector with word counts. This script also splits the data into train, validation, and test sets. TFlearn used as neural network for data analysis. Activation function of the output layer is ReLU.

Epigenetics-Answer-Classifier.ipynb on 5/24/17
Requirements:
import pandas as pd 
import json
import numpy as np
import tensorflow as tf
import tflearn
from tflearn.data_utils import to_categorical
import load_json
from collections import Counter
import spacy

Goal: convert answers into word vectors. The word vectors will have elements representing words in the total vocabulary.

Reads in answers and scores from a JSON file. In our pipeline, this is the output from json_load.py.
Can also read in data from a .csv file? Separate .csv files for separate data sets that can be analyzed at the same time?

This program first uses spaCy for natural language processing with the built-in standard english language model. SpaCy tokenizes the input data, and keeps the 10,000 most frequent words in the dataset to create a vocabulary, the framework for the word vectors. Then, for each student answer in the dataset, spaCy creates a word vector based on this vocabulary with word counts.

Then the program uses TensorFlow and TFlearn neural network packages to analyze the word vectors output from spaCy. First, it defines training, validation, and test data sets.

This will reserve 10% of the dataset for the test no matter what else happens with the run. Then the program takes a subset of info for training and a subset for validation, runs; shuffles the subsets; and runs again. Original settings have the program use 80% of the data as a training set and 10% as a validation set.

neural network info:

Input layer: need to input the size of the vocabulary derived from spaCy and being used in the analysis

Hidden layers: This adds a fully connected layer where every unit in the previous layer is connected to every unit in this layer. The hidden layers use the ReLU activation function in order to capture that the output of the model is not simply 0 or 1, but that the normalized score values can include a range from 0-1 (range of the actual scores depends on the input data … thus why we’re using normalized scores in the input. (Any other info on the ReLU function required in order to include this in paper?). The current version of the pipeline uses 2 hidden layers with 110 nodes total.

Output layer:

(To what degree is this standard for tensor flow and tflearn? Can these layers be altered? Are they altered as part of the program? To what degree are layers important for the function of the neural network and for other language or data scientists to understand what comes out of this network?)

Training
optimizer sets the training method, here stochastic gradient descent
learning_rate is the learning rate
loss determines how the network error is calculated. In this example, with the categorical cross-entropy.

Future updates to the neural network, or analyses to be run with the neural network:
#TODO: select the best activation function to predict scores that have values 0-12. Currently using ReLU as the activation function, but others might be more appropriate. Can then compare between activation functions to determine what generates the most accurate predictions with least error using this neural network on this dataset. More optimization needed before generalizing to other answer and score datasets.

When the network is run on the test set to measure performance, it calculates the mean squared error. (Why is this the performance metric? What results or ideas can we really take away from this? What range of values is “good” or “bad” or informative?)


Notes and questions about the code
Currently, in our pipeline, we’re working with vocabulary drawn from the words in the student answers only. We don’t have any given vocabulary that’s independent from student answers. In future versions of the pipeline, we want to input known vocabulary. Options: epigenetics based vocabulary, general biomedical vocabulary drawn from Pubmed citations/abstracts and/or publicly available papers. Want to them compare the predictive power of the neural network when it works just from the student answer vocabulary vs. the Pubmed vocabulary or some other BioNLP vocabulary. (What will this tell us about the predictive power of our NLP program? The neural network?)

Would the word frequency cutoffs need to be changed if using a different source for the vocabulary?

How easy is it to plug in other features to the model? Easy to do with Adolfo’s pipeline. Need to know the size of the input (how many elements in the vector, since it’s always a column vector)

Project steps

1.	Find JSON schema (structure of data)
2.	Extract question 1 answers and evals from JSON, create aggregate/average evals for each answer
3.	Generate vector space from all the text in the student answers - Q1 answer corpus (note: we’ll want to use the correct answer provided in the key as a model vector somehow)
4.	Vectorize answers with aggregate stores (incidence vectors)
5.	Split data into sets: train, test, validation
6.	Machine learning setup and pipeline
7.	Train and evaluate

Questions to think about:
How do deal with words that are not in the official answer? Can have good answers that don’t use the same words. We were originally thinking about generating the vector space from just the ideal answers. But instead going to generate it from the corpus of words to prevent this problem. Note that these vectors will be HUGE and variable.

Can use NLTK python library for removing “filler” words (important for grammar, not the keywords we want to look at in the answer). Can also use spaCy for this.


Can use Python JSON to create dictionary of the words in the dataset. Dylan’s suggestion. If you know how to do this, add notes about it.

At some point, may want to generate different representations of the data. For different features, want different chunks of the text. Class of features that is a bag of words. Others can take into account sentence-level features.


Natural language processing methods
spaCy
Natural language processing in Python: spaCy
https://spacy.io/

Spacy features we can use during this hackathon in bold
●	Non-destructive tokenization
●	Syntax-driven sentence segmentation
●	Pre-trained word vectors
●	Part-of-speech tagging
●	Named entity recognition
●	Labelled dependency parsing
●	Convenient string-to-int mapping
●	Export to numpy data arrays
●	GIL-free multi-threading
●	Efficient binary serialization
●	Easy deep learning integration
●	Statistical models for English and German
●	State-of-the-art speed
●	Robust, rigorously evaluated accuracy
spaCy resources
Source for spacy models:
https://github.com/explosion/spacy-models/blob/master/README.md

Tutorials:
https://spacy.io/docs/usage/tutorials

Spacy github page with example scripts: Quite straightforward, work from these
https://github.com/explosion/spaCy/tree/master/examples

Jupyter notebooks for spacy intros:
https://github.com/explosion/spacy-notebooks

After working with the default models, working with entity recognition might be useful. Good for differentiating between keywords we care about that are “simple” (CpG island) or “conceptual” (regulation).
https://spacy.io/docs/usage/entity-recognition

For after this hackathon: deep learning
https://spacy.io/docs/usage/deep-learning
https://explosion.ai/blog/deep-learning-formula-nlp
Triples would be great for distilling sentences and looking at sentence-level features of the answer data. Triples distills a sentence to subject, verb, object. Because it can reflect both cause and effect and the overall gist of the sentence it will likely reveal differences between answers that contain the right words but not the right concepts, and answers that link together concepts correctly and score highly.

About triples:
http://blog.cytora.com/insights/2016/11/21/extracting-information-from-natural-language-using-triples

Brief example on how to ask spaCy for triples in this tutorial:
https://nicschrading.com/project/Intro-to-NLP-with-spaCy/

May use spaCy for processing (getting rid of small words

Neural Network

TensorFlow
https://www.tensorflow.org/tutorials/
Punch cards
Completed tasks
Project outline presentation

5/22/17 To do list:
1.	Find host for Jupyter notebook
2.	Organize gitlab repository
3.	Share dataset privately
4.	Clean unicode (html to JSON can have artifacts we want to get rid of)

5/23/17, from 5/22/17 list
5.	Generate dummy dataset for gitlab
6.	Extract JSON into python dictionary

5/23/17
To-do after hackathon
To make the project publically available
Need to add stuff to the gitlab page so that it can actually be used by other people - what’s most important
Need to move this to a github page so that it’s easier - we need to permanently host this somewhere, and should set that page up today. Have Ivo do this.

According to the event organizer, Ben: “If there’s no useable github readme it doesn’t exist”
Can host this project on the following ncbi hackathon github pages. Talk to Ben to set up hosting
ncbi-hackathons.github.io
github.com/ncbi-hackathons

Additions to gitlab readme:

Update the description and goals section
Edit readme to make pipeline image visible - currently broken

Section on what the pipeline does with example results

Section on how the pipeline works with information on BioNLP, spaCy, neural networks, and TFlearn

To continue research and results
