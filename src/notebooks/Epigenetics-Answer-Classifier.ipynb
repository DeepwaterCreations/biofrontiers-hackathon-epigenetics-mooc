{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Epigenetics-MOOC Answer Classifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import json\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tflearn\n",
    "from tflearn.data_utils import to_categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the data\n",
    "\n",
    "The goal here is to convert the answers into word vectors. The word vectors will have elements representing words in the total vocabulary. \n",
    "\n",
    "The data needs to be preprocessed to use lower case characters. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the data\n",
    "\n",
    "Read the answers and the scores from the JSON file.\n",
    "\n",
    "Assuming the data was put into separate csv files, the calls will be.\n",
    "\n",
    "Can also read data directly from the JSON file into a dictionary.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#TODO: Put this in a csv file so we don't have to re-parse the json every time\n",
    "import load_json\n",
    "\n",
    "answers, scores = load_json.get_features(\"../../data/extractedRawDataJSON\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get word frequency distribution\n",
    "\n",
    "Count how often each word appears in the data set. \n",
    "\n",
    "Use this count to create a vocabulary to encode the answers and build the word vectors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total words in data set:  28268\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "word_counts = Counter()\n",
    "\n",
    "# TODO: Refactor this code to parse the 'answers' object.\n",
    "# The answers object is defined above.\n",
    "\n",
    "for idx, answer in enumerate(answers):\n",
    "        for word in answer.split(\" \"):\n",
    "            word_counts[word] += 1\n",
    "\n",
    "print(\"Total words in data set: \", len(word_counts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keep the 10000 most frequent words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['of', 'the', 'and', 'in', 'to', 'are', 'CpG', 'is', 'methylation', 'DNA', 'islands', 'regions', 'a', 'cancer', 'repetitive', '', 'genes', 'In', 'can', 'intergenic', 'elements', 'gene', 'that', 'at', 'be', 'normal', 'genomic', 'which', 'or', 'as', 'tumor', 'cells', 'these', 'suppressor', 'by', 'with', 'silencing', 'cell', 'The', 'promoters', 'This', 'this', 'methylated', 'hypermethylation', 'hypomethylation', 'for', 'genes.', 'expression', 'genome', 'repeats', 'tumour', 'also', 'activation', 'it', 'cells,', 'promoter', 'found', 'instability', 'cancer,', 'transcription']\n"
     ]
    }
   ],
   "source": [
    "vocab = sorted(word_counts, key=word_counts.get, reverse=True)[:10000]\n",
    "print(vocab[:60])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each answer in the data, create a word vector. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word2idx = {word : i for i, word in enumerate(vocab)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert Answers to Vectors\n",
    "\n",
    "This method takes a string of words (answer) as input and returns a vector with word counts. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def text_to_vector(text):\n",
    "    vector = np.zeros(len(vocab))\n",
    "    for w in text.split(' '):\n",
    "        indx = word2idx.get(w,None)\n",
    "        if indx == None:\n",
    "            continue\n",
    "        else:\n",
    "            vector[indx] += 1\n",
    "    return vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, run through the entire data set and convert each answer to a word vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectors = np.zeros((len(answers), len(vocab)), dtype=np.int_)\n",
    "for x, text in enumerate(answers):\n",
    "    word_vectors[x] = text_to_vector(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Train, Validation & Test sets\n",
    "\n",
    "Split the data into train, validation, and test sets. \n",
    "\n",
    "The function `to_categorical` from TFLearn reshapes the target data to use X output units."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 12.75    12.875   12.6875 ...,  12.8125  13.875   12.25  ]\n",
      "[[ 12.75    12.875   12.6875 ...,  12.8125  13.875   12.25  ]]\n"
     ]
    }
   ],
   "source": [
    "records = len(scores)\n",
    "\n",
    "shuffle = np.arange(records)\n",
    "np.random.shuffle(shuffle)\n",
    "test_fraction = 0.9\n",
    "\n",
    "train_split, test_split = shuffle[:int(records*test_fraction)], shuffle[int(records*test_fraction):]\n",
    "trainX, trainY = word_vectors[train_split,:], np.array(list( scores[i] for i in train_split ))\n",
    "testX, testY = word_vectors[test_split,:], list( scores[i] for i in test_split )\n",
    "\n",
    "print(trainY)\n",
    "# print(trainY.shape)\n",
    "trainY.shape = (1,len(train_split))\n",
    "print(trainY)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12.375, 10.375, 11.25, 13.0, 12.5625, 11.25, 13.4375, 13.0, 1.75, 10.6875, 13.75, 12.375, 13.6875, 13.625, 13.25, 12.4375, 12.75, 12.3125, 12.375, 13.125, 13.625, 13.4375, 11.875, 11.5, 11.875, 11.75, 14.0, 11.6875, 12.25, 12.25, 13.375, 13.9375, 13.5625, 13.75, 6.875, 13.625, 10.5, 13.875, 8.625, 11.25, 11.875, 11.75, 12.25, 12.125, 14.0, 12.75, 13.5, 11.375, 13.25, 12.375, 9.625, 11.375, 6.375, 10.125, 13.8125, 10.125, 12.75, 11.5, 10.125, 13.375, 13.875, 13.625, 13.375, 12.0625, 13.25, 11.25, 12.4375, 11.9375, 13.25, 6.1875, 11.0625, 13.25, 7.59375, 13.3125, 13.3125, 12.875, 9.875, 13.75, 10.375, 12.1875, 11.375, 13.5, 7.375, 13.75, 12.875, 13.125, 10.125, 10.75, 11.5, 8.625, 11.5, 12.375, 12.125, 10.375, 12.0, 13.75, 11.6875, 12.25, 13.625, 13.25, 13.125, 12.875, 12.125, 12.875, 12.125, 12.125, 11.6875, 11.375, 13.875, 9.625, 12.25, 12.625, 13.375, 13.75, 13.375, 12.6875, 12.875, 11.875, 13.75, 11.59375, 11.375, 9.4375, 6.125, 13.125, 13.8125, 11.625, 13.5, 13.8125, 13.625, 13.5, 13.25, 14.0, 11.0, 12.25, 10.4375, 12.75, 7.125, 13.875, 13.0625, 13.0, 10.625, 13.5, 10.25, 13.75, 12.25, 11.5, 13.0, 10.34375, 8.5, 13.5, 3.0625, 12.5625, 13.875, 10.9375, 13.5, 13.875, 11.875, 5.625, 14.0, 10.0, 13.0, 12.25, 11.375, 13.0, 10.0, 6.4375, 11.0625, 10.375, 12.5, 7.5, 10.625, 13.4375, 10.75, 0.4375, 12.0, 12.0, 13.875, 11.1875, 12.4375, 10.75, 10.875, 7.5625, 9.8125, 13.1875, 13.875, 13.0, 13.8125, 6.875, 10.0625, 9.53125, 13.875, 11.0, 13.5, 12.4375, 12.0625, 12.4375, 7.0, 12.5, 12.0625, 14.0, 13.25, 12.25, 13.4375, 10.625, 12.9375, 13.875, 7.375, 12.09375, 12.5, 13.5, 11.75, 6.75, 11.5, 14.0, 13.375, 14.0, 12.0625, 9.75, 8.875, 13.375, 12.0, 8.25, 13.125, 11.875, 13.75, 11.25, 11.5, 12.375, 12.375, 6.75, 12.5, 13.0, 13.125, 8.875, 11.8125, 14.0, 13.9375, 13.75, 7.25, 10.9375, 11.125, 13.5, 11.125, 6.875, 11.125, 13.0, 13.625, 14.0, 13.625, 14.0, 13.5625, 14.0, 12.6875, 13.25, 13.75, 13.875, 13.0, 13.9375, 13.125, 13.625, 7.25, 12.0625, 13.75, 12.375, 12.4375, 9.40625, 14.0, 7.0, 12.25, 8.5, 13.125, 5.3125, 10.625, 13.4375, 12.6875, 13.0, 9.75, 12.0, 13.75, 13.875, 11.5, 0.0, 13.0, 13.375, 14.0, 10.875, 13.25, 13.875, 11.3125, 12.25, 13.125, 11.9375, 13.25, 11.625, 13.0, 13.375, 10.625, 13.5, 12.875, 8.75, 7.25, 13.25, 13.625, 13.25, 10.9375, 9.375, 11.3125, 12.375, 11.0, 14.0, 10.125, 12.9375, 12.75, 13.625, 13.25, 13.375, 14.0, 11.875, 12.75, 12.0, 13.5, 12.125, 13.75, 13.625, 6.8125, 14.0, 12.75, 13.5, 10.25, 10.75, 13.625, 11.0, 4.625, 8.75, 11.5, 11.3125, 10.25, 11.0625, 11.9375, 12.875, 13.0, 13.875, 12.4375, 13.75, 12.625, 12.875, 11.375, 13.75, 13.0, 13.0, 12.6875, 1.375, 12.0, 12.75, 11.375, 10.375, 13.0, 13.75, 12.625, 13.0, 12.625, 12.375, 13.9375, 12.625, 11.75, 11.375, 12.0, 13.5, 12.625, 13.3125, 13.5, 13.75, 12.625, 11.625, 13.75, 11.875, 8.5, 12.25, 11.625, 13.0, 13.375, 12.0625, 10.0, 12.75, 14.0, 12.75, 12.375, 11.0, 13.5, 10.25, 11.875, 10.9375, 8.25, 11.0, 11.9375, 13.875, 8.5, 12.625, 12.375, 10.5, 13.875, 12.125, 8.125, 12.875, 13.375, 10.0, 12.5, 13.25, 12.1875, 12.25, 11.875, 14.0, 13.75, 6.75, 9.6875, 14.0, 9.75, 13.25, 11.25, 14.0, 8.5, 13.0625, 13.75, 10.125, 13.5625, 13.875, 5.25, 13.875, 12.375, 12.5, 9.625, 13.5, 13.75, 11.75, 13.625, 12.3125, 12.75, 3.375, 13.9375, 13.0, 13.3125, 6.0, 9.0, 11.875, 13.0, 13.5, 12.625, 3.8125, 14.0, 11.0, 9.25, 12.375, 13.5, 12.375, 13.125, 12.625, 13.6875, 6.84375, 13.125, 9.75, 11.125, 8.0625, 11.75, 12.5, 12.25, 13.5, 13.75, 10.75, 13.75, 13.5, 13.9375, 12.0, 13.375, 10.25, 11.875, 13.125, 12.0, 12.75, 13.375, 11.75, 10.625, 10.25, 9.625, 12.0625, 12.875, 12.375, 10.84375, 10.0, 6.03125, 12.5, 13.0, 11.25, 11.0625, 12.0, 13.1875, 13.125, 13.75, 11.625, 13.5, 10.375, 14.0, 13.625, 11.25, 13.5, 12.125, 9.75, 14.0, 13.625, 12.0, 11.0, 13.125, 12.5, 6.75, 12.9375, 13.125, 13.1875, 13.5, 10.5, 11.125, 9.625, 13.5, 12.0, 11.6875, 10.25, 12.875, 13.875, 11.875, 10.375, 12.25, 12.0, 11.875, 11.0625, 11.1875, 11.875, 6.5, 13.75, 14.0, 13.875, 11.5, 13.75, 6.3125, 12.5, 13.375, 12.4375, 8.875, 13.46875, 14.0, 13.1875, 13.375, 5.125, 8.375, 11.5625, 14.0, 13.5625, 12.3125, 12.625, 13.875, 13.875, 12.5625, 13.375, 12.4375, 12.9375, 12.125, 14.0, 13.0, 10.4375, 14.0, 13.875, 12.125, 14.0, 13.0, 11.0, 12.25, 12.625, 11.0, 13.25, 13.375, 8.75, 12.25, 14.0, 10.625, 10.875, 13.375, 13.875, 12.5, 9.375, 13.625, 13.875, 11.8125, 12.0, 13.40625, 10.0, 13.125, 11.9375, 8.8125, 13.875, 12.5, 12.375, 12.625, 12.0, 11.625, 13.875, 13.0, 13.625, 13.0, 12.125, 12.375, 13.0, 13.625, 13.4375, 13.125, 12.375, 12.625, 13.0, 11.4375, 12.625, 12.5625, 13.75, 14.0, 13.25, 11.8125, 10.75, 14.0, 13.75, 12.0, 11.25, 11.25, 11.75, 14.0, 14.0, 11.875, 13.9375, 12.5, 13.5, 11.375, 13.0, 13.21875, 8.875, 13.125, 13.6875, 11.0, 13.625, 10.0, 11.125, 12.5, 14.0, 13.0, 10.9375, 9.0, 11.5, 12.75, 12.8125, 10.875, 11.0625, 13.75, 13.0, 12.75, 12.5, 13.5, 12.625, 12.90625, 13.25, 11.5, 12.625, 13.625, 11.875, 10.6875, 9.75, 13.5625, 9.0, 13.375, 12.3125, 13.75, 13.125, 13.0, 13.3125, 13.625, 11.5, 12.375, 8.375, 12.375, 13.8125, 5.625, 11.75, 13.25, 12.25, 11.5, 11.0, 12.5, 11.5, 10.5, 13.5, 13.1875, 13.0, 11.0, 13.125, 12.3125, 14.0, 13.5, 11.75, 13.25, 12.03125, 13.125, 10.375, 12.875, 6.25, 3.75, 12.75, 12.75, 13.625, 11.25, 13.0, 13.6875, 11.9375, 12.5, 14.0, 13.25, 9.75, 12.5, 14.0, 5.625, 12.5, 3.875, 13.75, 10.25, 13.9375, 13.5, 3.28125, 11.5, 13.625, 12.5625, 8.0, 13.25, 13.625, 8.3125, 8.25, 10.25, 2.125, 12.6875, 13.625, 14.0, 13.25, 10.28125, 13.375, 5.8125, 13.0, 11.125, 13.90625, 10.625, 13.0, 11.25, 6.875, 13.125, 12.375, 13.75, 12.625, 12.625, 9.75, 13.875, 10.375, 12.875, 13.5, 13.875, 14.0, 8.25, 8.875, 12.125, 12.375, 9.8125, 11.6875, 13.75, 11.125, 11.25, 14.0, 13.4375, 13.5, 13.75, 11.25, 12.5, 10.5625, 13.5, 13.75, 13.5, 11.6875, 3.375, 13.5, 13.375, 11.75, 13.1875, 13.3125, 8.75, 7.8125, 13.0, 12.625, 12.9375, 13.375, 10.875, 13.0, 14.0, 9.125, 13.3125, 12.75, 6.375, 8.375, 13.125, 11.4375, 14.0, 12.875, 13.25, 13.9375, 13.125, 13.125, 13.875, 14.0, 8.5, 12.75, 14.0, 13.0625, 13.5, 13.1875, 7.75, 13.1875, 13.0625, 12.5, 13.5, 10.125, 3.75, 9.75, 7.75, 13.75, 9.875, 11.3125, 13.125, 11.75, 13.75, 13.25, 13.6875, 11.9375, 5.375, 13.25, 10.875, 11.875, 12.25, 13.5, 12.75, 14.0, 6.96875, 13.875, 13.1875, 13.125, 9.5625, 12.5, 12.875, 9.5, 13.125, 11.25, 10.75, 10.75, 13.375, 10.0, 12.8125, 13.75, 9.375, 11.875, 13.4375, 8.4375, 12.625, 13.25, 13.375, 13.75, 6.125, 13.75, 10.125, 11.875, 13.375, 13.875, 12.875, 13.0, 13.5, 11.9375, 12.875, 9.75, 13.25, 13.8125, 13.375, 9.0, 12.625, 9.3125, 12.875, 10.5, 12.625, 11.375, 12.96875, 7.25, 0.0, 13.25, 12.125, 13.5, 13.75, 13.875, 11.125, 7.625, 11.75, 8.0625, 12.5, 8.125, 13.5, 13.125, 13.75, 13.75, 13.75, 11.125, 11.3125, 13.75, 13.625, 13.625, 12.75, 13.75, 12.8125, 13.6875, 9.75, 8.0, 11.78125, 11.875, 5.125, 12.625, 12.125, 10.0, 13.25, 13.6875, 11.25, 13.875, 12.5, 8.9375, 8.0, 13.3125, 11.625, 11.125, 13.5, 11.625, 13.625, 11.5625, 12.375, 8.75, 10.75, 12.125, 8.75, 7.875, 12.8125, 13.5, 14.0, 13.375, 12.875, 14.0, 14.0, 12.5625, 13.25, 13.125, 9.5, 11.625, 8.625, 13.5, 11.25, 13.125, 14.0, 12.125, 13.9375, 13.75, 13.4375, 3.5, 8.5, 8.375, 13.75, 10.5, 8.625, 3.5, 13.125, 10.5625, 11.5, 13.375, 12.75, 11.75, 13.25, 11.625, 12.625, 6.5, 12.3125, 13.5, 13.625, 12.6875, 11.5, 10.75, 10.8125, 12.0, 13.875, 13.5625, 14.0, 11.75, 10.875, 12.25, 10.875, 11.6875, 12.375, 11.75, 13.5, 12.125, 12.25, 12.1875, 12.5, 13.375, 10.1875, 13.75, 12.625, 13.5, 10.875, 12.25, 9.25, 13.0625, 12.1875, 13.125, 13.75, 12.75, 10.0, 13.125, 12.125, 12.125, 13.4375, 5.75, 12.875, 13.9375, 12.75, 12.5, 11.375, 13.125, 8.375, 6.3125, 13.25, 13.75, 13.75, 7.375, 13.75, 13.25, 14.0, 13.5, 12.5, 8.75, 13.25, 10.3125, 13.75, 14.0, 10.875, 12.625, 12.875, 4.125, 12.3125, 11.125, 10.0, 13.625, 12.375, 11.375, 11.25, 13.4375, 11.625, 10.5, 10.75, 10.5, 12.375, 13.25, 13.125, 11.875, 13.375, 12.875, 9.875, 12.1875, 12.25, 9.1875, 13.5, 13.375, 12.5, 11.125, 10.375, 12.84375, 13.0, 13.5, 13.625, 8.9375, 5.125, 12.375, 9.25, 12.3125, 13.125, 13, 12.625, 14.0, 14.0, 13.25, 12.125, 13.875, 13.75, 11.125, 13.25, 13.5, 11.75, 10.0, 14.0, 6.375, 13.625, 9.375, 13.5, 12.5, 10.125, 11.5, 11.5625, 12.625, 13.0, 13.3125, 13.0, 12.3125, 9.5, 13.875, 13.625, 10.375, 11.0, 13.9375, 8.125, 12.625, 4.0, 13.75, 13.125, 12.0, 10.875, 12.5, 13.75, 12.375, 12.75, 9.9375, 14.0, 11.0, 13.125, 12.8125, 13.5625, 13.625, 6.125, 12.5, 8.125, 12.5, 13.5, 13.5, 13.875, 12.125, 13.375, 13.0625, 11.75, 13.5, 11.0, 13.125, 12.5, 10.125, 10.5, 12.5, 13.3125, 13.5, 13.625, 13.125, 13.9375, 13.375, 13.0, 12.125, 11.875, 13.25, 12.75, 14.0, 7.75, 13.1875, 12.8125, 13.5, 11.25, 14.0, 13.0, 13.25, 13.625, 11.125, 8.75, 10.625, 13.875, 8.0, 14.0, 12.25, 12.8125, 13.75, 11.75, 12.875, 13.25, 11.625, 9.25, 12.5, 13.5, 14.0, 12.875, 13.0, 12.5, 11.625, 13.375, 13.6875, 10.875, 13.375, 13.875, 12.75, 13.875, 13.5625, 12.625, 12.53125, 13.8125, 11.5, 6.1875, 13.375, 14.0, 12.125, 9.875, 14.0, 14.0, 13.875, 12.625, 11.625, 13.25, 14.0, 11.125, 12.75, 9.5, 11.25, 11.875, 13.875, 13.625, 12.9375, 13.8125, 13.375, 12.875, 12.0, 10.0, 10.375, 13.0, 12.8125, 13.375, 13.5, 14.0, 13.75, 10.5, 6.625, 10.875, 13.375, 13.5, 13.875, 11.5, 10.9375, 7.8125, 13.125, 12.875, 13.375, 13.375, 12.25, 9.0, 9.625, 7.0625, 13.375, 12.5, 10.75, 12.5, 12.625, 12.0, 13.4375, 13.125, 11.0625, 9.5, 13.0625, 13.25, 10.8125, 13.875, 12.375, 14.0, 10.8125, 13.75, 13.0, 9.75, 12.125, 11.0, 12.5, 10.0, 10.25, 13.5, 14.0, 12.625, 5.6875, 12.75, 11.75, 13.625, 10.75, 11.625, 7.75, 14.0, 13.5, 9.5, 13.75, 9.625, 12.25, 11.25, 10.0625, 8.0, 12.875, 12.875, 13.125, 12.875, 8.875, 12.5, 13.0, 13.5, 13.0, 4.25, 14.0, 13.875, 12.25, 9.375, 13.625, 13.125, 13.0, 11.5, 12.125, 13.25, 12.375, 12.625, 11.9375, 10.4375, 13.625, 12.1875, 12.375, 12.84375, 13.375, 13.125, 13.5, 6.875, 11.75, 11.125, 12.9375, 3.3125, 13.0, 12.6875, 13.125, 10.375, 9.125, 8.625, 0.625, 10.9375, 12.625, 8.5625, 10.09375, 13.375, 13.375, 7.75, 10.75, 12.25, 13.125, 9.75, 12.375, 13.75, 13.75, 12.375, 14.0, 12.25, 11.25, 13.5, 8.5, 13.75, 11.5, 13.0, 13.875, 6.75, 13.5, 12.375, 13.625, 14.0, 12.9375, 13.125, 14.0, 10.625, 13.75, 9.375, 9.6875, 10.75, 13.03125, 8.625, 12.375, 12.375, 13.3125, 12.25, 13.125, 12.0, 13.625, 11.75, 8.75, 12.75, 13.625, 9.125, 12.4375, 6.25, 13.0, 14.0, 11.375, 13.375, 6.4375, 13.9375, 9.75, 12.4375, 12.625, 13.25, 8.75, 12.34375, 8.5, 9.75, 13.0, 13.125, 12.0, 13.75, 8.25, 9.8125, 12.8125, 7.0, 14.0, 12.75, 13.0, 12.0, 11.75, 4.75, 13.5, 13.875, 12.5, 12.375, 13.6875, 11.375, 9.5, 13.625, 13.875, 12.8125, 13.875, 8.625, 11.75, 13.625, 13.625, 4.875, 11.5625, 13.75, 13.25, 13.1875, 13.25, 12.875, 12.25, 12.4375, 13.5, 12.625, 10.625, 12.75, 10.875, 13.1875, 14.0, 8.875, 11.375, 10.375, 13.5625, 11.0, 10.75, 13.0625, 11.9375, 10.875, 14.0, 12.25, 11.875, 13.5, 11.25, 14.0, 13.875, 13.125, 13.875, 12.25, 13.75, 11.8125, 12.5, 11.625, 12.375, 1.375, 13.6875, 12.625, 12.75, 7.6875, 11.0, 11.75, 10.4375, 13.5, 13.375, 12.5, 14.0, 12.375, 12.125, 12.9375, 13.125, 11.8125, 11.5, 10.625, 13.4375, 7.75, 12.5, 13.5, 12.0, 12.0, 9.875, 13.375, 13.5, 11.875, 13.75, 13.5, 12.375, 12.25, 13.875, 13.875, 10.09375, 9.1875, 14.0, 11.75, 12.125, 12.5, 8.0625, 12.5, 5.25, 12.25, 13.4375, 13.0, 12.125, 11.5, 13.75, 13.8125, 12.625, 14.0, 0.375, 8.25, 12.75, 13.5, 11.25, 10.4375, 9.5, 10.625, 12.5, 13.0, 12.75, 5.0, 12.25, 14.0, 10.25, 14.0, 11.875, 12.1875, 13.6875, 13.5, 11.0, 11.1875, 11.125, 14.0, 12.75, 11.75, 12.5, 0.0, 13.9375, 13.875, 12.125, 11.625, 12.875, 13.375, 13.3125, 13.75, 12.125, 13.75, 13.375, 13.25, 13.75, 12.375, 10.875, 13.375, 12.875, 13.6875, 9.875, 14.0, 12.75, 13.375, 12.25, 10.25, 13.0, 14.0, 12.25, 14.0, 12.5, 11.625, 12.625, 11.25, 13.5, 13.9375, 12.5, 13.5, 13.25, 12.25, 11.875, 10.125, 11.75, 11.75, 11.25, 13.625, 5.25, 13.0625, 13.8125, 11.875, 13.1875, 14.0, 5.6875, 4.0, 9.6875, 11.625, 11.125, 12.875, 13.25, 13.8125, 14.0, 11.9375, 13.75, 0.25, 13.25, 13.75, 11.125, 12.75, 13.5625, 13.0, 14.0, 12.875, 10.8125, 14.0, 13.0, 13.8125, 13.3125, 14.0, 12.25, 8.75, 12.625, 9.375, 11.625, 13.75, 12.875, 9.625, 13.8125, 13.125, 9.25, 13.1875, 13.75, 12.875, 13.5, 13.5625, 11.875, 10.375, 11.625, 12.375, 11.625, 12.125, 13.5, 12.5, 12.375, 8.875, 13.875, 12.1875, 12.3125, 13.6875, 13.0, 2.375, 10.625, 12.375, 13.625, 11.875, 11.6875, 5.625, 13.25, 11.375, 0.0, 13.125, 11.625, 13.125, 13.25, 9.5, 12.75, 2.75, 13.875, 11.3125, 13.625, 13.625, 14.0, 13.375, 9.0, 13.25, 10.625, 10.0625, 13.5, 13.375, 12.8125, 11.625, 14.0, 13.125, 10.625, 14.0, 13.0, 12.375, 13.875, 8.8125, 13.375, 13.375, 7.3125, 13.375, 9.1875, 8.875, 13.9375, 11.9375, 6.6875, 11.375, 4.5625, 12.125, 12.3125, 13.1875, 14.0, 11.0, 8.25, 14.0, 14.0, 12.625, 11.125, 13.75, 13.5, 11.5, 13.25, 13.6875, 13.75, 10.8125, 7.9375, 12.5, 10.1875, 13.625, 6.625, 12.5, 13.125, 13.25, 13.4375, 13.25, 8.25, 10.875, 12.625, 11.625, 13.4375, 12.0, 12.3125, 12.625, 9.875, 14.0, 5.625, 13.5, 12.375, 10.6875, 5.5625, 12.4375, 8.0, 12.25, 5.0, 5.1875, 12.25, 13.0, 12.875, 13.5, 11.6875, 13.0, 14.0, 11.875, 9.8125, 12.75, 12.25, 12.125, 9.25, 13.75, 12.5, 12.25, 13.875, 12.0, 7.5, 11.4375, 10.9375, 13.75, 14.0, 13.125, 9.0625, 12.125, 13.125, 12.5, 11.375, 13.5, 12.75, 7.0625, 7.96875, 12.5625, 11.9375, 13.0, 10.125, 11.125, 10.375, 13.375, 12.75, 10.0625, 13.125, 13.125, 13.25, 13.1875, 3.65625, 5.75, 13.25, 11.25, 12.0, 3.5, 1.625, 8.75, 11.75, 12.0, 2.875, 12.0, 13.0, 13.875, 12.875, 8.75, 13.625, 8.5, 12.875, 12.0, 12.625, 12.1875, 13.75, 12.625, 13.75, 12.0625, 12.875, 12.125, 13.75, 10.4375, 12.625, 12.75, 13.4375, 13.0625, 12.125, 10.25, 13.125, 11.0, 10.5, 11.25, 12.125, 12.0, 13.25, 13.75, 13.5, 10.625, 14.0, 13.5, 13.75, 12.875, 12.375, 11.75, 13.6875, 9.125, 13.625, 12.75, 10.625, 12.125, 10.5, 11.5, 0.0, 13.25, 7.5, 12.6875, 12.25, 12.375, 10.125, 6.5, 14.0, 13.0, 12.875, 13.25, 13.625, 11.0, 13.125, 12.25, 11.625, 8.75, 13.5, 13.375, 11.8125, 13.125, 12.75, 13.5, 13.25, 13.75, 10.75, 12.9375, 12.75, 13.5, 7.25, 7.6875, 13.5, 13.25, 12.5, 13.75, 13.25, 11.5, 13.4375, 12.25, 7.6875, 13.375, 14.0, 13.0, 13.0, 14.0, 11.5625, 12.0, 12.125, 11.25, 13.5, 14.0, 9.8125, 10.25, 11.625, 13.75, 13.5, 10.375, 12.875, 8.75, 13.25, 10.1875, 14.0, 13.875, 13.75, 13.8125, 12.0, 13.875, 12.0, 14.0, 0.5, 13.625, 12.5, 9.75, 11.5, 13.5, 10.625, 13.625, 12.625, 13.5, 11.625, 11.625, 13.625, 12.125, 12.5625, 9.625, 13.5, 9.75, 6.75, 13.75, 13.75, 11.3125, 14.0, 13.3125, 8.5, 13.25, 13.375, 4.8125, 11.25, 13.0, 9.75, 13.0, 11.625, 12.5, 12.125, 12.8125, 12.375, 4.0, 13.0, 13.75, 13.5, 9.8125, 7.25, 9.6875, 13.25, 12.25, 12.75, 13.0, 4.625, 12.4375, 10.9375, 13.5, 13.625, 11.125, 3.25, 13.875, 9.375, 14.0, 12.25, 9.5625, 14.0, 12.75, 13.28125, 12.625, 13.5, 12.125, 13.0, 13.75, 12.875, 12.25, 13.3125, 12.6875, 14.0, 13.0, 12.875, 12.25, 7.3125, 13.0, 14.0, 9.125, 11.625, 13.75, 10.75, 10.75, 13.125, 10.375, 12.75, 10.875, 12.125, 8.75, 12.5, 11.75, 13.875, 9.375, 13.0, 13.0, 13.4375, 8.375, 13.0, 8.21875, 13.625, 13.75, 13.75, 13.0, 13.3125, 7.0, 12.0, 12.125, 9.375, 13.8125, 12.5625, 12.25, 12.125, 3.125, 13.1875, 7.5, 11.625, 12.75, 13.625, 14.0, 13.875, 13.75, 9.75, 12.75, 13.03125, 14.0, 13.5, 12.875, 12.75, 12.125, 13.625, 7.625, 10.0, 13.25, 5.75, 2.75, 13.5, 11.25, 12.875, 13.5, 13.375, 13.875, 13.25, 14.0, 14.0, 13.25, 13.75, 10.875, 13.75, 11.0625, 4.3125, 11.125, 9.75, 9.875, 10.5625, 2.125, 3.5, 11.0, 13.125, 12.75, 9.3125, 11.875, 13.5625, 12.125, 13.875, 12.375, 13.25, 12.625, 11.5, 14.0, 13.375, 10.5, 13.3125, 12.0, 9.6875, 12.625, 12.875, 13.625, 8.6875, 11.625, 12.75, 12.75, 13.625, 12.625, 10.375, 11.125, 12.21875, 12.875, 13.4375, 9.75, 8.25, 9.5, 12.75, 12.0, 10.9375, 5.8125, 10.125, 12.875, 4.625, 12.875, 8.25, 13.125, 10.3125, 14.0, 12.1875, 12.0625, 12.0, 13.8125, 14.0, 12.09375, 12.75, 13.75, 13.0, 12.625, 13.75, 11.875, 13.5, 8.75, 13.0, 11.5, 13.0, 6.625, 14.0, 10.375, 13.875, 14.0, 13.9375, 13.25, 10.875, 11.5, 10.375, 11.0625, 8.5, 12.625, 7.5, 8.4375, 5.875, 6.0, 12.75, 12.25, 6.5625, 9.25, 8.5, 11.25, 13.125, 13.75, 10.75, 13.75, 14.0, 12.75, 10.75, 14.0, 13.4375, 14.0, 5.875, 11.125, 9.625, 13.125, 7.75, 13.25, 12.875, 5.875, 10.625, 9.25, 5.0625, 9.4375, 13.625, 6.125, 13.4375, 12.875, 11.9375, 5.125, 13.25, 13.625, 12.0, 13.625, 13.25, 11.0, 11.625, 12.25, 6.375, 11.8125, 12.625, 12.5, 13.0, 12.6875, 12.125, 13.25, 13.3125, 7.375, 12.5625, 13.5, 9.875, 12.0, 12.125, 13.25, 13.3125, 12.125, 8.125, 10.625, 13.0, 13.875, 12.875, 13.375, 13.0, 12.25, 12.5625, 13.25, 12.8125, 10.75, 11.0, 3.125, 3.9375, 11.125, 11.0, 13.9375, 12.875, 14.0, 14.0, 13.375, 12.75, 10.0, 14.0, 11.25, 10.0, 13.5625, 12.9375, 12.25, 12.375, 13.5, 2.9375, 13.0, 8.1875, 12.0625, 14.0, 13.3125, 13.9375, 11.375, 14.0, 5.4375, 11.125, 5.625, 13.875, 13.875, 12.75, 12.375, 11.0, 12.5, 13.125, 14.0, 12.75, 12.25, 13.5, 11.5, 13.5, 13.375, 13.625, 12.0, 9.625, 12.75, 13.375, 9.0, 11.875, 13.375, 12.375, 12.875, 12.625, 8.0, 13.625, 13.875, 11.4375, 13.125, 8.0, 12.5, 13.5, 13.0625, 10.5, 13.25, 12.25, 13.625, 10.875, 12.25, 7.0, 12.3125, 12.0, 12.875, 10.5, 14.0, 13.3125, 13.6875, 4.875, 13.875, 13.75, 14.0, 11.875, 9.5, 10.3125, 12.625, 7.5, 13.0, 10.625, 13.125, 13.625, 10.875, 11.875, 13.625, 10.75, 11.375, 12.75, 10.375, 13.875, 13.625, 2.8125, 11.9375, 13.875, 10.6875, 9.375, 13.75, 12.3125, 13.875, 8.46875, 13.6875, 14.0, 9.5, 12.9375, 8.125, 2.5, 12.5, 10.0, 13.4375, 11.875, 12.5, 8.875, 8.875, 12.4375, 11.25, 12.5, 12.5, 6.75, 14.0, 13.75, 13.75, 8.0, 11.0, 13.0625, 13.0, 2.125, 10.5, 14.0, 11.25, 13.25, 13.375, 9.1875, 11.75, 10.5, 13.375, 4.625, 12.9375, 12.875, 13.375, 13.125, 12.25, 11.625, 9.9375, 13.625, 13.875, 11.75, 13.875, 13.3125, 9.0, 14.0, 4.9375, 13.3125, 12.125, 13.875, 7.75, 8.125, 8.625, 11.375, 12.375, 13.5625, 13.375, 3.125, 9.25, 7.125, 9.4375, 13.25, 11.75, 13.0, 9.75, 13.75, 12.25, 13.625, 12.125, 13.75, 12.375, 12.625, 11.3125, 13.0, 12.125, 13.125, 13.125, 13.125, 8.75, 10.875, 12.0, 12.75, 12.5, 12.375, 11.75, 8.75, 12.3125, 10.375, 13.25, 12.25, 13.625, 12.125, 13.125, 13.25, 13.625, 9.09375, 13.875, 10.25, 10.0, 12.125, 11.3125, 5.875, 12.25, 12.75, 13.625, 13.5, 13.75, 11.75, 13.6875, 11.3125, 4.5, 5.5, 13.375, 14.0, 12.5, 13.0, 12.25, 10.0625, 13.375, 2.125, 9.75, 10.25, 12.0, 12.8125, 13.75, 14.0, 13.4375, 13.0, 11.625, 13.75, 7.125, 13.625, 12.75, 10.25, 7.625, 13.875, 14.0, 13.25, 11.5625, 13.875, 12.9375, 10.0625, 11.375, 12.875, 14.0, 13.625, 11.5625, 12.875, 12.375, 14.0, 12.0, 11.625, 9.875, 11.6875, 13.25, 14.0, 12.375, 13.0, 9.0, 13.5, 5.75, 13.4375, 13.125, 11.75, 13.875, 12.0, 0.0, 13.5, 12.625, 13.625, 12.875, 12.375, 10.5, 8.6875, 12.125, 13.4375, 10.75, 11.5, 13.0, 12.4375, 12.75, 12.625, 13.375, 7.5, 13.125, 13.0, 13.0, 12.875, 10.5, 13.5, 8.125, 13.375, 13.3125, 12.0, 13.75, 11.875, 13.625, 13.875, 11.0625, 13.875, 14.0, 13.375, 8.25, 12.5, 9.0, 7.125, 13.75, 12.25, 12.0, 8.625, 12.5, 13.375, 11.375, 9.625, 9.625, 12.0, 11.375, 7.625, 10.375, 12.25, 12.75, 12.125, 13.875, 12.625, 13.25, 12.75, 12.5, 12.25, 10.5, 11.875, 12.625, 13.6875, 9.0, 10.125, 11.25, 12.125, 8.25, 13.75, 13.875, 9.875, 11.875, 13.25, 6.0, 11.625, 13.0, 13.3125, 11.875, 13.125, 7.75, 13.375, 6.25, 13.625, 7.75, 13.6875, 11.40625, 13.625, 10.625, 13.3125, 13.0, 12.75, 11.125, 12.375, 12.875, 13.75, 14.0, 13.9375, 13.0, 6.25, 13.9375, 12.875, 12.5, 13.375, 14.0, 10.375, 12.625, 12.6875, 13.0, 11.625, 11.375, 13.75, 13.375, 14.0, 12.1875, 8.4375, 12.75, 14.0, 13.625, 12.4375, 8.0, 14.0, 7.125, 9.0, 2.875, 11.5, 3.375, 11.75, 9.3125, 13.375, 13.125, 13.125, 14.0, 13.875, 6.9375, 8.125, 11.25, 11.09375, 12.625, 12.75, 14.0, 9.5, 11.125, 11.75, 11.9375, 13.5625, 13.125, 14.0, 11.90625, 11.25, 12.75, 11.25, 13.375, 11.25, 12.375, 11.9375, 10.125, 13.5, 13.375, 9.5625, 10.0, 9.75, 13.75, 13.375, 13.75, 11.125, 13.5, 13.375, 13.25, 13.5, 13.5, 7.5, 13.0, 12.375, 13.375, 11.5, 14.0, 10.75, 13.875, 6.4375, 10.875, 11.625, 13.375, 9.0, 5.28125, 10.375, 14.0, 11.625, 13.625, 13.875, 14.0, 12.25, 13.5, 12.875, 11.9375, 13.25, 11.0, 13.25, 13.125, 5.1875, 6.3125, 6.9375, 13.375, 13.375, 12.875, 12.125, 12.25, 11.625, 12.0, 12.4375, 12.6875, 10.5, 13.0, 13.625, 10.3125, 11.375, 12.5, 12.875, 13.125, 12.875, 12.875, 13.5, 12.9375, 11.375, 13.375, 13.125, 10.625, 10.625, 8.875, 12.25, 11.875, 12.75, 11.5, 12.625, 13.0, 12.25, 12.875, 8.9375, 3.5, 13.5, 13.875, 12.5, 12.875, 8.1875, 11.875, 11.625, 7.5, 11.78125, 10.8125, 13.375, 12.5, 12.75, 12.0, 13.125, 9.875, 13.0, 9.375, 12.4375, 13.1875, 13.0, 13.625, 13.75, 10.8125, 12.5, 9.5625, 13.6875, 8.25, 8.84375, 11.75, 14.0, 10.21875, 13.375, 10.3125, 13.375, 13.5, 3.875, 13.75, 11.0, 13.5, 0.125, 12.75, 9.5, 11.875, 12.75, 13.75, 13.125, 8.5, 13.5, 6.5, 13.875, 13.75, 12.125, 12.125, 13.75, 12.1875, 12.0, 13.125, 13.875, 13.75, 13.0, 6.125, 13.875, 12.75, 11.0, 13.625, 11.125, 13.75, 13.75, 13.75, 13.3125, 12.625, 13.25, 13.75, 12.125, 12.5, 9.625, 9.75, 11.25, 13.25, 13.0, 13.5, 13.375, 13.25, 10.75, 12.75, 11.0, 9.6875, 12.5, 11.75, 13.0, 9.375, 2.625, 12.875, 12.5, 13.375, 12.5, 12.75, 6.125, 12.5625, 13.25, 10.25, 13.0, 13.0, 4.3125, 13.0, 3.0, 13.375, 13.375, 10.875, 13.0, 13.5, 11.75, 11.75, 11.3125, 9.4375, 11.125, 13.625, 12.25, 11.0, 13.875, 13.875, 14.0, 12.9375, 12.5, 11.5, 9.875, 7.875, 13.875, 13.5625, 12.125, 14.0, 6.84375, 8.25, 13.25, 11.25, 12.1875, 12.0, 13.5, 9.875, 13.5625, 13.5, 11.25, 12.75, 13.5, 11.875, 4.125, 8.5625, 14.0, 13.75, 11.25, 12.75, 11.25, 12.5, 9.125, 13.1875, 12.25, 13.6875, 12.75, 13.375, 9.375, 12.625, 9.9375, 13.5, 12.375, 6.28125, 10.375, 10.25, 13.875, 13.5, 12.125, 13.625, 13.625, 10.625, 13.375, 11.5, 12.75, 13.875, 11.875, 11.9375, 9.5, 12.875, 13.5, 13.5, 12.5, 14.0, 10.625, 10.3125, 10.1875, 13.5, 12.375, 12.5, 12.0, 6.125, 12.375, 11.625, 12.90625, 13.5, 10.8125, 13.375, 7.9375, 13.625, 11.875, 13.125, 5.5625, 13.1875, 12.75, 12.25, 5.375, 11.1875, 9.375, 13.875, 13.625, 11.75, 8.9375, 12.375, 9.8125, 13.4375, 12.125, 13.5, 13.25, 13.25, 10.0, 10.25, 12.75, 13.0625, 13.5, 14.0, 12.5, 7.0, 12.875, 13.28125, 11.875, 10.375, 8.34375, 8.625, 13.5, 13.5, 6.3125, 12.75, 13.0, 11.75, 13.25, 12.0, 13.8125, 13.25, 11.125, 8.8125, 12.3125, 10.125, 12.125, 11.875, 14.0, 9.5, 11.8125, 11.625, 12.5, 13.125, 13.75, 9.875, 13.75, 10.5, 13.5, 13.75, 13.5, 13.375, 12.625, 13.6875, 12.25, 13.5, 9.3125, 1.5, 13.3125, 11.0, 13.375, 13.75, 13.875, 12.5, 9.5, 12.25, 12.75, 4.1875, 13.0, 11.625, 10.5, 11.75, 13.0, 12.5, 12.0, 11.25, 12.5, 11.625, 13.25, 13.25, 13.1875, 12.125, 10.0, 5.1875, 9.75, 10.25, 11.125, 11.75, 12.9375, 13.625, 13.125, 12.75, 13.5, 13.0, 13.875, 10.125, 6.75, 11.5, 13.625, 13.625, 14.0, 13.9375, 11.0, 13.6875, 7.5, 7.375, 13.75, 11.625, 14.0, 13.5, 11.625, 12.75, 10.0, 6.25, 11.0, 11.625, 10.5, 13.125, 10.3125, 13.0, 12.125, 8.875, 12.875, 11.0, 11.75, 13.875, 10.625, 13.125, 13.25, 14.0, 11.9375, 12.875, 12.625, 12.125, 11.8125, 13.25, 11.8125, 9.8125, 11.75, 7.75, 12.5625, 14.0, 7.0, 12.875, 12.5, 13.5, 13.0, 14.0, 12.625, 7.6875, 13.0625, 12.5625, 11.875, 11.25, 12.25, 11.0, 8.0, 12.75, 14.0, 11.875, 13.125, 12.5625, 13.75, 12.5, 13.375, 13.5, 13.625, 12.9375, 13.5, 9.875, 12.875, 7.9375, 11.6875, 9.75, 12.5, 6.375, 12.25, 11.9375, 7.6875, 14.0, 13.3125, 9.25, 13.875, 8.875, 14.0, 13.625, 7.6875, 13.375, 9.875, 13.0, 12.5, 12.25, 12.25, 12.125, 13.875, 9.5, 13.0, 8.75, 9.6875, 12.0, 11.5, 11.25, 11.9375, 10.375, 13.875, 12.8125, 6.5, 13.0, 10.0, 13.875, 13.9375, 13.625, 8.4375, 13.875, 12.625, 13.5, 12.5625, 11.25, 13.875, 12.125, 13.25, 13.625, 12.125, 13.875, 6.0, 10.0, 12.125, 11.75, 12.875, 13.5, 12.25, 13.25, 13.75, 11.375, 11.8125, 11.03125, 11.375, 13.25, 12.25, 2.5, 12.25, 11.375, 14.0, 11.5, 13.625, 11.75, 12.6875, 13.0, 7.25, 13.125, 13.9375, 9.875, 12.0, 11.5625, 12.375, 11.25, 10.5, 13.3125, 11.0, 9.75, 12.6875, 14.0, 13.25, 13.9375, 10.0625, 12.875, 5.875, 14.0, 9.6875, 12.625, 12.75, 13.875, 9.875, 12.25, 12.875, 12.75, 11.9375, 12.09375, 12.0, 11.375, 12.25, 10.6875, 12.25, 13.5, 13.25, 10.875, 10.75, 12.5, 10.125, 13.25, 9.625, 12.3125, 14.0, 12.875, 13.5, 13.25, 11.75, 12.75, 8.375, 13.8125, 13.75, 10.875, 7.8125, 13.0, 13.0, 11.1875, 13.375, 12.4375, 13.625, 8.125, 9.375, 6.625, 11.0, 12.125, 12.75, 12.3125, 12.375, 13.25, 12.625, 10.25, 13.1875, 11.5, 12.0, 13.4375, 11.0, 12.375, 12.6875, 12.125, 11.5, 12.25, 11.1875, 12.625, 12.75, 13.75, 12.9375, 10.625, 12.5, 12.1875, 13.0, 10.25, 1.625, 5.625, 12.125, 11.875, 12.625, 9.75, 13.5, 13.625, 8.625, 13.625, 11.625, 13.125, 12.0625, 11.5, 10.875, 4.875, 11.375, 12.9375, 13.0, 11.5, 12.6875, 11.0625, 10.375, 12.125, 13.25, 7.9375, 4.6875, 11.25, 7.9375, 9.625, 11.6875, 13.75, 13.25, 13.625, 13.125, 13.1875, 12.5625, 3.5, 12.625, 9.0, 12.8125, 11.0, 10.375, 11.75, 13.875, 13.875, 13.75, 9.8125, 13.5, 13.625, 12.6875, 13.625, 12.25, 13.0, 12.5, 13.0, 13.25, 10.5, 13.5, 13.5, 13.125, 10.8125, 11.0, 14.0, 13.9375, 12.75, 13.6875, 12.5, 14.0, 13.875, 13.5, 12.75, 8.5625, 11.75, 9.125, 9.375, 11.25, 12.5625, 8.875, 10.625, 12.75, 9.125, 11.25, 13.625, 9.0, 12.125, 11.875, 13.125, 13.25, 12.25, 12.375, 13.5, 11.75, 12.4375, 11.25, 8.6875, 10.625, 9.0, 14.0, 13.875, 13.5, 13.625, 11.75, 13.625, 12.6875, 9.0625, 13.5, 10.5, 9.1875, 11.875]\n"
     ]
    }
   ],
   "source": [
    "print(trainY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The network\n",
    "\n",
    "### Input layer\n",
    "\n",
    "Must provide the number of input units. For our problem, `n_input_units` is the size of the epigenetics vocabulary. \n",
    "\n",
    "Setting the first argument to `None` chooses the default batch size.\n",
    "\n",
    "```\n",
    "net = tflearn.input_data([None, n_input_units])\n",
    "```\n",
    "\n",
    "### Hidden layers\n",
    "\n",
    "Add hidden layers with \n",
    "\n",
    "```\n",
    "net = tflearn.fully_connected(net, n_units, activation='ReLU')\n",
    "```\n",
    "\n",
    "This adds a fully connected layer where every unit in the previous layer is connected to every unit in this layer. \n",
    "\n",
    "Arguments:\n",
    "`net` the network created with the call to `tflearn.input_data`. This tells the network to use the output of the previous layer as the input to this layer. \n",
    "`n_units`: the number of units in the layer.\n",
    "`activation`: the activation function. \n",
    "\n",
    "Add more hidden layers by repeatedly calling `net = tflearn.fully_connected(net, n_units)`.\n",
    "\n",
    "### Output layer\n",
    "\n",
    "The last layer you add is used as the output layer. \n",
    "\n",
    "Set the number of units to match the target data. In our case the score is a single number. We need only one output unit.\n",
    "\n",
    "```\n",
    "net = tflearn.fully_connected(net, 1, activation='ReLU')\n",
    "```\n",
    "\n",
    "### Training\n",
    "\n",
    "To set how you train the network, use \n",
    "\n",
    "```\n",
    "net = tflearn.regression(net, optimizer='sgd', learning_rate=0.1, loss='categorical_crossentropy')\n",
    "```\n",
    "\n",
    "Arguments: \n",
    "\n",
    "* `optimizer` sets the training method, here stochastic gradient descent\n",
    "* `learning_rate` is the learning rate\n",
    "* `loss` determines how the network error is calculated. In this example, with the categorical cross-entropy.\n",
    "\n",
    "Finally, create the model with `tflearn.DNN(net)`. So it ends up looking something like \n",
    "\n",
    "```\n",
    "net = tflearn.input_data([None, X])                          # Input\n",
    "net = tflearn.fully_connected(net, 5, activation='ReLU')      # Hidden\n",
    "net = tflearn.fully_connected(net, 1, activation='ReLU')   # Output\n",
    "net = tflearn.regression(net, optimizer='sgd', learning_rate=0.1, loss='categorical_crossentropy')\n",
    "model = tflearn.DNN(net)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Build the Neural Network\n",
    "def build_net():\n",
    "    # Reset all parameters and variables. Use it if you are using Jupyter\n",
    "    tf.reset_default_graph()\n",
    "\n",
    "    \n",
    "    # Input layer\n",
    "    # Set the number of input units to be equal to the size of the epigenetics vocabulary\n",
    "    n_input_units = len(vocab)\n",
    "    net = tflearn.input_data([None, n_input_units])\n",
    "    \n",
    "    # Hidden layers\n",
    "    net = tflearn.fully_connected(net, 10, activation='ReLU') \n",
    "    net = tflearn.fully_connected(net, 5, activation='ReLU')\n",
    "    \n",
    "    # Output\n",
    "    # Number of units is defined by the structure of the score  \n",
    "    # Setting it to 1 for first iteration of the application where the score is a float (0-12)\n",
    "    n_output_units = 1\n",
    "    \n",
    "    \n",
    "    # Network parameters\n",
    "    # optimizer: the training method, here stochastic gradient descent\n",
    "    # learning_rate: is the learning rate\n",
    "    # loss` determines how the network error is calculated. In this example, with the categorical cross-entropy.\n",
    "    #TODO: select the best activation function to predict scores that have values 0-12.\n",
    "    net = tflearn.fully_connected(net, n_output_units, activation='ReLU')\n",
    "    net = tflearn.regression(net, optimizer='sgd', learning_rate=0.1, loss='categorical_crossentropy')\n",
    "       \n",
    "    nn = tflearn.DNN(net)\n",
    "    return nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intialize the Neural Network\n",
    "\n",
    "`build_net()` builds the model. \n",
    "\n",
    "Add arguments if you want to change parameters in the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = build_net()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the network\n",
    "\n",
    "Now that we've constructed the network, saved as the variable `model`, we can fit it to the data. \n",
    "\n",
    "Use the `model.fit` method to train the network. \n",
    "\n",
    "`trainX`: training features  \n",
    "`trainY`: training targets . \n",
    "`validation_set=0.1`: reserves 10% of the data set as the validation set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-81-39923f4bc03b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Train the network\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_set\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshow_metric\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home/adolfo/anaconda3/envs/biof/lib/python3.6/site-packages/tflearn/models/dnn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X_inputs, Y_targets, n_epoch, validation_set, show_metric, batch_size, shuffle, snapshot_epoch, snapshot_step, excl_trainops, validation_batch_size, run_id, callbacks)\u001b[0m\n\u001b[1;32m    181\u001b[0m         \u001b[0;31m# TODO: check memory impact for large data and multiple optimizers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m         feed_dict = feed_dict_builder(X_inputs, Y_targets, self.inputs,\n\u001b[0;32m--> 183\u001b[0;31m                                       self.targets)\n\u001b[0m\u001b[1;32m    184\u001b[0m         \u001b[0mfeed_dicts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfeed_dict\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_ops\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m         \u001b[0mval_feed_dicts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/adolfo/anaconda3/envs/biof/lib/python3.6/site-packages/tflearn/utils.py\u001b[0m in \u001b[0;36mfeed_dict_builder\u001b[0;34m(X, Y, net_inputs, net_targets)\u001b[0m\n\u001b[1;32m    281\u001b[0m                 \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    282\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 283\u001b[0;31m                 \u001b[0mfeed_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnet_inputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    284\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m             \u001b[0;31m# If a dict is provided\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "# Train the network\n",
    "model.fit(trainX, trainY, validation_set=0.1, show_metric=True, batch_size=128, n_epoch=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing\n",
    "\n",
    "Run the network on the test set to measure its performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: Adjust this test\n",
    "predictions = (np.array(model.predict(testX))[:,0] >= 0.5).astype(np.int_)\n",
    "test_accuracy = np.mean(predictions == testY[:,0], axis=0)\n",
    "print(\"Accuracy: \", test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
