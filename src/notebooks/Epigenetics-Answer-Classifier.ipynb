{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Epigenetics-MOOC Answer Classifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import json\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tflearn\n",
    "from tflearn.data_utils import to_categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the data\n",
    "\n",
    "The goal here is to convert the answers into word vectors. The word vectors will have elements representing words in the total vocabulary. \n",
    "\n",
    "The data needs to be preprocessed to use lower case characters. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the data\n",
    "\n",
    "Read the answers and the scores from the JSON file.\n",
    "\n",
    "Assuming the data was put into separate csv files, the calls will be.\n",
    "\n",
    "Can also read data directly from the JSON file into a dictionary.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#TODO: Put this in a csv file so we don't have to re-parse the json every time\n",
    "import load_json\n",
    "\n",
    "answers, scores = load_json.get_features(\"../../data/extractedRawDataJSON\", True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get word frequency distribution\n",
    "\n",
    "Count how often each word appears in the data set. \n",
    "\n",
    "Use this count to create a vocabulary to encode the answers and build the word vectors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total words in data set:  12331\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en\")\n",
    "proc_answers = []\n",
    "for ans in answers:\n",
    "    doc = nlp.tokenizer(ans)\n",
    "    proc_ans = [tok.lower_ for tok in doc if tok.is_alpha and not tok.is_stop]\n",
    "    proc_answers.append(\" \".join(proc_ans))\n",
    "\n",
    "word_counts = Counter()\n",
    "\n",
    "for idx, answer in enumerate(proc_answers):\n",
    "        for word in answer.split(\" \"):\n",
    "            word_counts[word] += 1\n",
    "\n",
    "print(\"Total words in data set: \", len(word_counts))\n",
    "\n",
    "\n",
    "# print(proc_answers[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keep the 10000 most frequent words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cpg', 'methylation', 'cancer', 'genes', 'dna', 'islands', 'regions', 'cells', 'elements', 'repetitive', 'gene', 'intergenic', 'normal', 'methylated', 'tumor', 'genomic', 'cell', 'silencing', 'genome', 'promoters', 'suppressor', 'hypomethylation', 'expression', 'hypermethylation', 'repeats', 'instability', 'tumour', 'hypermethylated', 'activation', 'hypomethylated', 'transcription', 'promoter', 'island', 'recombination', 'usually', 'normally', 'lead', 'leads', 'illegitimate', 'stability', 'cryptic', 'unmethylated', 'growth', 'epigenetic', 'silenced', 'associated', 'function', 'sites', 'disruption', 'transposition', 'specific', 'cause', 'result', 'occurs', 'insertions', 'deletions', 'tend', 'wide', 'translocations', 'transcriptional']\n"
     ]
    }
   ],
   "source": [
    "vocab = sorted(word_counts, key=word_counts.get, reverse=True)\n",
    "print(vocab[:60])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each answer in the data, create a word vector. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word2idx = {word : i for i, word in enumerate(vocab)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert Answers to Vectors\n",
    "\n",
    "This method takes a string of words (answer) as input and returns a vector with word counts. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def text_to_vector(text):\n",
    "    vector = np.zeros(len(vocab))\n",
    "    for w in text.split(' '):\n",
    "        indx = word2idx.get(w,None)\n",
    "        if indx == None:\n",
    "            continue\n",
    "        else:\n",
    "            vector[indx] += 1\n",
    "    return vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, run through the entire data set and convert each answer to a word vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/adolfo/anaconda3/envs/biof/lib/python3.6/site-packages/sklearn/utils/validation.py:429: DataConversionWarning: Data with input dtype int64 was converted to float64 by the normalize function.\n",
      "  warnings.warn(msg, _DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "word_vectors = np.zeros((len(answers), len(vocab)), dtype=np.int_)\n",
    "for x, text in enumerate(answers):\n",
    "    word_vectors[x] = text_to_vector(text)\n",
    "    \n",
    "#Normalize word vectors\n",
    "from sklearn import preprocessing\n",
    "word_vectors = preprocessing.normalize(word_vectors, norm='l2')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Train, Validation & Test sets\n",
    "\n",
    "Split the data into train, validation, and test sets. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "records = len(scores)\n",
    "\n",
    "shuffle = np.arange(records)\n",
    "np.random.shuffle(shuffle)\n",
    "test_fraction = 0.9\n",
    "\n",
    "train_split, test_split = shuffle[:int(records*test_fraction)], shuffle[int(records*test_fraction):]\n",
    "trainX, trainY = word_vectors[train_split,:], np.array(list( scores[i] for i in train_split ))\n",
    "testX, testY = word_vectors[test_split,:], np.array(list( scores[i] for i in test_split ))\n",
    "\n",
    "trainY.shape = (len(train_split),1)\n",
    "testY.shape = (len(test_split),1)\n",
    "#print(trainY)\n",
    "#print(testY)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#print(trainY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The network\n",
    "\n",
    "### Input layer\n",
    "\n",
    "Must provide the number of input units. For our problem, `n_input_units` is the size of the epigenetics vocabulary. \n",
    "\n",
    "Setting the first argument to `None` chooses the default batch size.\n",
    "\n",
    "```\n",
    "net = tflearn.input_data([None, n_input_units])\n",
    "```\n",
    "\n",
    "### Hidden layers\n",
    "\n",
    "Add hidden layers with \n",
    "\n",
    "```\n",
    "net = tflearn.fully_connected(net, n_units, activation='ReLU')\n",
    "```\n",
    "\n",
    "This adds a fully connected layer where every unit in the previous layer is connected to every unit in this layer. \n",
    "\n",
    "Arguments:\n",
    "`net` the network created with the call to `tflearn.input_data`. This tells the network to use the output of the previous layer as the input to this layer. \n",
    "`n_units`: the number of units in the layer.\n",
    "`activation`: the activation function. \n",
    "\n",
    "Add more hidden layers by repeatedly calling `net = tflearn.fully_connected(net, n_units)`.\n",
    "\n",
    "### Output layer\n",
    "\n",
    "The last layer you add is used as the output layer. \n",
    "\n",
    "Set the number of units to match the target data. In our case the score is a single number. We need only one output unit.\n",
    "\n",
    "```\n",
    "net = tflearn.fully_connected(net, 1, activation='ReLU')\n",
    "```\n",
    "\n",
    "### Training\n",
    "\n",
    "To set how you train the network, use \n",
    "\n",
    "```\n",
    "net = tflearn.regression(net, optimizer='sgd', learning_rate=0.1, loss='categorical_crossentropy')\n",
    "```\n",
    "\n",
    "Arguments: \n",
    "\n",
    "* `optimizer` sets the training method, here stochastic gradient descent\n",
    "* `learning_rate` is the learning rate\n",
    "* `loss` determines how the network error is calculated. In this example, with the categorical cross-entropy.\n",
    "\n",
    "Finally, create the model with `tflearn.DNN(net)`. So it ends up looking something like \n",
    "\n",
    "```\n",
    "net = tflearn.input_data([None, X])                          # Input\n",
    "net = tflearn.fully_connected(net, 5, activation='ReLU')      # Hidden\n",
    "net = tflearn.fully_connected(net, 1, activation='ReLU')   # Output\n",
    "net = tflearn.regression(net, optimizer='sgd', learning_rate=0.1, loss='mean_square')\n",
    "model = tflearn.DNN(net)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Build the Neural Network\n",
    "def build_net(activation, learning_rate=0.1, loss='mean_square',hidden_units_1=100, hidden_units_2=10):\n",
    "    # Reset all parameters and variables. Use it if you are using Jupyter\n",
    "    tf.reset_default_graph()\n",
    "\n",
    "    \n",
    "    # Input layer\n",
    "    # Set the number of input units to be equal to the size of the epigenetics vocabulary\n",
    "    n_input_units = len(vocab)\n",
    "    net = tflearn.input_data([None, n_input_units])\n",
    "    \n",
    "    # Hidden layers\n",
    "    # Use ReLU as default activation for the hidden units\n",
    "    \n",
    "    net = tflearn.fully_connected(net, hidden_units_1, activation='ReLU') \n",
    "    net = tflearn.fully_connected(net, hidden_units_2, activation='ReLU')\n",
    "    \n",
    "    # Output layer  \n",
    "    # Set output units to 1 b/c the score is a float [0-12+] normalized to [0,1]\n",
    "    n_output_units = 1\n",
    "    net = tflearn.fully_connected(net, n_output_units, activation=activation)\n",
    "    \n",
    "    # Network parameters\n",
    "    # optimizer: the training method\n",
    "    # learning_rate: \n",
    "    # loss`: determines how the network error is calculated..\n",
    " \n",
    "    net = tflearn.regression(net, optimizer='sgd', learning_rate=learning_rate, loss=loss)\n",
    "       \n",
    "    nn = tflearn.DNN(net, tensorboard_verbose=3)\n",
    "    return nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intialize the Neural Network\n",
    "\n",
    "`build_net()` builds the model. \n",
    "\n",
    "Add arguments if you want to change parameters in the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_net('ReLU', 0.001,'mean_square', 100, 10 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the network\n",
    "\n",
    "Now that we've constructed the network, saved as the variable `model`, we can fit it to the data. \n",
    "\n",
    "Use the `model.fit` method to train the network. \n",
    "\n",
    "`trainX`: training features  \n",
    "`trainY`: training targets . \n",
    "`validation_set=0.1`: reserves 10% of the data set as the validation set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 30999  | total loss: \u001b[1m\u001b[32m0.02819\u001b[0m\u001b[0m | time: 2.587s\n",
      "| SGD | epoch: 1000 | loss: 0.02819 - binary_acc: 0.0673 -- iter: 3000/3098\n",
      "Training Step: 31000  | total loss: \u001b[1m\u001b[32m0.02806\u001b[0m\u001b[0m | time: 3.680s\n",
      "| SGD | epoch: 1000 | loss: 0.02806 - binary_acc: 0.0646 | val_loss: 0.02560 - val_acc: 0.0783 -- iter: 3098/3098\n",
      "--\n"
     ]
    }
   ],
   "source": [
    "# Train the network\n",
    "model.fit(trainX, trainY, validation_set=0.1, show_metric=True, batch_size=100, n_epoch=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing\n",
    "\n",
    "Run the network on the test set to measure its performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE:  0.0273765052856\n"
     ]
    }
   ],
   "source": [
    "predictions = np.array(model.predict(testX))[:,0]\n",
    "# Calculate the Mean Squared Error\n",
    "mse = ((testY[:,0] - predictions) ** 2).mean(axis=0)\n",
    "print(\"MSE: \", mse)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
