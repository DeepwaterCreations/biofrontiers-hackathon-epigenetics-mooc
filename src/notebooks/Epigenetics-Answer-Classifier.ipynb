{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Epigenetics-MOOC Answer Classifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-54bb2b31bee5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtflearn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtflearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mto_categorical\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import json\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tflearn\n",
    "from tflearn.data_utils import to_categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the data\n",
    "\n",
    "The goal here is to convert the answers into word vectors. The word vectors will have elements representing words in the total vocabulary. \n",
    "\n",
    "The data needs to be preprocessed to use lower case characters. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the data\n",
    "\n",
    "Read the answers and the scores from the JSON file.\n",
    "\n",
    "Assuming the data was put into separate csv files, the calls will be.\n",
    "\n",
    "Can also read data directly from the JSON file into a dictionary.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Put this in a csv file so we don't have to re-parse the json every time\n",
    "import load_json\n",
    "\n",
    "answers, scores = load_json.get_features(\"../../data/extractedRawDataJSON\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get word frequency distribution\n",
    "\n",
    "Count how often each word appears in the data set. \n",
    "\n",
    "Use this count to create a vocabulary to encode the answers and build the word vectors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "word_counts = Counter()\n",
    "\n",
    "# TODO: Refactor this code to parse the 'answers' object.\n",
    "# The answers object is defined above.\n",
    "\n",
    "for idx, answer in enumerate(answers):\n",
    "        for word in answer.split(\" \"):\n",
    "            word_counts[w] += 1\n",
    "\n",
    "#print(\"Total words in data set: \", len(word_counts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keep the 10000 most frequent words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['D']\n"
     ]
    }
   ],
   "source": [
    "vocab = sorted(word_counts, key=word_counts.get, reverse=True)[:10000]\n",
    "print(vocab[:60])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each answer in the data, create a word vector. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word2idx = {word : i for i, word in enumerate(vocab)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert Answers to Vectors\n",
    "\n",
    "This method takes a string of words (answer) as input and returns a vector with word counts. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def text_to_vector(text):\n",
    "    vector = np.zeros(len(vocab))\n",
    "    for w in text.split(' '):\n",
    "        indx = word2idx.get(w,None)\n",
    "        if indx == None:\n",
    "            continue\n",
    "        else:\n",
    "            vector[indx] += 1\n",
    "    return vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, run through the entire data set and convert each answer to a word vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_vectors = np.zeros((len(answers), len(vocab)), dtype=np.int_)\n",
    "for x, (_, text) in enumerate(answers.iterrows()):\n",
    "    word_vectors[x] = text_to_vector(text[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Train, Validation & Test sets\n",
    "\n",
    "Split the data into train, validation, and test sets. \n",
    "\n",
    "The function `to_categorical` from TFLearn reshapes the target data to use X output units."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "records = len(scores)\n",
    "\n",
    "shuffle = np.arange(records)\n",
    "np.random.shuffle(shuffle)\n",
    "test_fraction = 0.9\n",
    "\n",
    "train_split, test_split = shuffle[:int(records*test_fraction)], shuffle[int(records*test_fraction):]\n",
    "trainX, trainY = word_vectors[train_split,:], to_categorical(scores[train_split], 1)\n",
    "testX, testY = word_vectors[test_split,:], to_categorical(scores[test_split], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The network\n",
    "\n",
    "### Input layer\n",
    "\n",
    "Must provide the number of input units. For our problem, `n_input_units` is the size of the epigenetics vocabulary. \n",
    "\n",
    "Setting the first argument to `None` chooses the default batch size.\n",
    "\n",
    "```\n",
    "net = tflearn.input_data([None, n_input_units])\n",
    "```\n",
    "\n",
    "### Hidden layers\n",
    "\n",
    "Add hidden layers with \n",
    "\n",
    "```\n",
    "net = tflearn.fully_connected(net, n_units, activation='ReLU')\n",
    "```\n",
    "\n",
    "This adds a fully connected layer where every unit in the previous layer is connected to every unit in this layer. \n",
    "\n",
    "Arguments:\n",
    "`net` the network created with the call to `tflearn.input_data`. This tells the network to use the output of the previous layer as the input to this layer. \n",
    "`n_units`: the number of units in the layer.\n",
    "`activation`: the activation function. \n",
    "\n",
    "Add more hidden layers by repeatedly calling `net = tflearn.fully_connected(net, n_units)`.\n",
    "\n",
    "### Output layer\n",
    "\n",
    "The last layer you add is used as the output layer. \n",
    "\n",
    "Set the number of units to match the target data. In our case the score is a single number. We need only one output unit.\n",
    "\n",
    "```\n",
    "net = tflearn.fully_connected(net, 1, activation='ReLU')\n",
    "```\n",
    "\n",
    "### Training\n",
    "\n",
    "To set how you train the network, use \n",
    "\n",
    "```\n",
    "net = tflearn.regression(net, optimizer='sgd', learning_rate=0.1, loss='categorical_crossentropy')\n",
    "```\n",
    "\n",
    "Arguments: \n",
    "\n",
    "* `optimizer` sets the training method, here stochastic gradient descent\n",
    "* `learning_rate` is the learning rate\n",
    "* `loss` determines how the network error is calculated. In this example, with the categorical cross-entropy.\n",
    "\n",
    "Finally, create the model with `tflearn.DNN(net)`. So it ends up looking something like \n",
    "\n",
    "```\n",
    "net = tflearn.input_data([None, X])                          # Input\n",
    "net = tflearn.fully_connected(net, 5, activation='ReLU')      # Hidden\n",
    "net = tflearn.fully_connected(net, 1, activation='ReLU')   # Output\n",
    "net = tflearn.regression(net, optimizer='sgd', learning_rate=0.1, loss='categorical_crossentropy')\n",
    "model = tflearn.DNN(net)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Build the Neural Network\n",
    "def build_net():\n",
    "    # Reset all parameters and variables. Use it if you are using Jupyter\n",
    "    tf.reset_default_graph\n",
    "    \n",
    "    # Input layer\n",
    "    # Set the number of input units to be equal to the size of the epigenetics vocabulary\n",
    "    n_input_units = len(vocab)\n",
    "    net = tflearn.input_data([None, n_input_units])\n",
    "    \n",
    "    # Hidden layers\n",
    "    net = tflearn.fully_connected(net, 10, activation='ReLU') \n",
    "    net = tflearn.fully_connected(net, 5, activation='ReLU')\n",
    "    \n",
    "    # Output\n",
    "    # Number of units is defined by the structure of the score  \n",
    "    # Setting it to 1 for first iteration of the application where the score is a float (0-12)\n",
    "    n_output_units = 1\n",
    "    \n",
    "    \n",
    "    # Network parameters\n",
    "    # optimizer: the training method, here stochastic gradient descent\n",
    "    # learning_rate: is the learning rate\n",
    "    # loss` determines how the network error is calculated. In this example, with the categorical cross-entropy.\n",
    "    #TODO: select the best activation function to predict scores that have values 0-12.\n",
    "    net = tflearn.fully_connected(net, n_output_units, activation='ReLU')\n",
    "    net = tflearn.regression(net, optimizer='sgd', learning_rate=0.1, loss='categorical_crossentropy')\n",
    "       \n",
    "    nn = tflearn.DNN(net)\n",
    "    return nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intialize the Neural Network\n",
    "\n",
    "`build_net()` builds the model. \n",
    "\n",
    "Add arguments if you want to change parameters in the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = build_net()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the network\n",
    "\n",
    "Now that we've constructed the network, saved as the variable `model`, we can fit it to the data. \n",
    "\n",
    "Use the `model.fit` method to train the network. \n",
    "\n",
    "`trainX`: training features  \n",
    "`trainY`: training targets . \n",
    "`validation_set=0.1`: reserves 10% of the data set as the validation set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Train the network\n",
    "model.fit(trainX, trainY, validation_set=0.1, show_metric=True, batch_size=128, n_epoch=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing\n",
    "\n",
    "Run the network on the test set to measure its performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: Adjust this test\n",
    "predictions = (np.array(model.predict(testX))[:,0] >= 0.5).astype(np.int_)\n",
    "test_accuracy = np.mean(predictions == testY[:,0], axis=0)\n",
    "print(\"Accuracy: \", test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
